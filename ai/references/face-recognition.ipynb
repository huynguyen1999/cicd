{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import face_recognition\n",
    "from face_recognition.face_recognition_cli import image_files_in_folder\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from sklearn import neighbors\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n",
    "    \"\"\"\n",
    "    Trains a k-nearest neighbors classifier for face recognition.\n",
    "\n",
    "    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n",
    "\n",
    "     (View in source code to see train_dir example tree structure)\n",
    "\n",
    "     Structure:\n",
    "        <train_dir>/\n",
    "        ├── <person1>/\n",
    "        │   ├── <somename1>.jpeg\n",
    "        │   ├── <somename2>.jpeg\n",
    "        │   ├── ...\n",
    "        ├── <person2>/\n",
    "        │   ├── <somename1>.jpeg\n",
    "        │   └── <somename2>.jpeg\n",
    "        └── ...\n",
    "\n",
    "    :param model_save_path: (optional) path to save model on disk\n",
    "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n",
    "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
    "    :param verbose: verbosity of training\n",
    "    :return: returns knn classifier that was trained on the given data.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Loop through each person in the training set\n",
    "    for class_dir in os.listdir(train_dir):\n",
    "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
    "            continue\n",
    "\n",
    "        # Loop through each training image for the current person\n",
    "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
    "            image = face_recognition.load_image_file(img_path)\n",
    "            face_bounding_boxes = face_recognition.face_locations(image)\n",
    "\n",
    "            if len(face_bounding_boxes) != 1:\n",
    "                # If there are no people (or too many people) in a training image, skip the image.\n",
    "                if verbose:\n",
    "                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n",
    "            else:\n",
    "                # Add face encoding for current image to the training set\n",
    "                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n",
    "                y.append(class_dir)\n",
    "\n",
    "    # Determine how many neighbors to use for weighting in the KNN classifier\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int(round(math.sqrt(len(X))))\n",
    "        if verbose:\n",
    "            print(\"Chose n_neighbors automatically:\", n_neighbors)\n",
    "\n",
    "    # Create and train the KNN classifier\n",
    "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
    "    knn_clf.fit(X, y)\n",
    "\n",
    "    # Save the trained KNN classifier\n",
    "    if model_save_path is not None:\n",
    "        with open(model_save_path, 'wb') as f:\n",
    "            pickle.dump(knn_clf, f)\n",
    "\n",
    "    return knn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Recognizes faces in given image using a trained KNN classifier\n",
    "\n",
    "    :param X_img_path: path to image to be recognized\n",
    "    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n",
    "    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n",
    "    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n",
    "           of mis-classifying an unknown person as a known one.\n",
    "    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n",
    "        For faces of unrecognized persons, the name 'unknown' will be returned.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n",
    "        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n",
    "\n",
    "    if knn_clf is None and model_path is None:\n",
    "        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n",
    "\n",
    "    # Load a trained KNN model (if one was passed in)\n",
    "    if knn_clf is None:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            knn_clf = pickle.load(f)\n",
    "\n",
    "    # Load image file and find face locations\n",
    "    X_img = face_recognition.load_image_file(X_img_path)\n",
    "    X_face_locations = face_recognition.face_locations(X_img)\n",
    "\n",
    "    # If no faces are found in the image, return an empty result.\n",
    "    if len(X_face_locations) == 0:\n",
    "        return []\n",
    "\n",
    "    # Find encodings for faces in the test iamge\n",
    "    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n",
    "\n",
    "    # Use the KNN model to find the best matches for the test face\n",
    "    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=2)\n",
    "    print(closest_distances)\n",
    "    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n",
    "\n",
    "    # Predict classes and remove classifications that aren't within the threshold\n",
    "    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction_labels_on_image(img_path, predictions):\n",
    "    \"\"\"\n",
    "    Shows the face recognition results visually.\n",
    "\n",
    "    :param img_path: path to image to be recognized\n",
    "    :param predictions: results of the predict function\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    font = ImageFont.load_default()\n",
    "    for name, (top, right, bottom, left) in predictions:\n",
    "        # Draw a box around the face using the Pillow module\n",
    "        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n",
    "\n",
    "        # # There's a bug in Pillow where it blows up with non-UTF-8 text\n",
    "        # # when using the default bitmap font\n",
    "        # name = name.encode(\"UTF-8\")\n",
    "\n",
    "        # # Draw a label with a name below the face\n",
    "        # text_height = 10\n",
    "        # draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n",
    "        # draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n",
    "        font = ImageFont.load_default()\n",
    "        draw.text((0, 0),name, fill=(255,0,0), font=font)\n",
    "\n",
    "    # Remove the drawing library from memory as per the Pillow docs\n",
    "    del draw\n",
    "\n",
    "    # Display the resulting image\n",
    "    display(pil_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = train('./images', model_save_path=\"trained_knn_model.clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.53843062, 0.64654359]]), array([[2, 1]]))\n"
     ]
    }
   ],
   "source": [
    "predictions = predict('./unknown-trump.jpeg', knn_clf=knn_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/admin/Source/FullStackLearning/web-chat/ai/references/face-recognition.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/admin/Source/FullStackLearning/web-chat/ai/references/face-recognition.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prediction_labels_on_image('./unknown-trump.jpeg', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_images = {\n",
    "    \"trump\": [\n",
    "        \"./images/trump/trump1.jpg\"\n",
    "    ],\n",
    "    \"ariana\":[\n",
    "        \"./images/ariana/ariana1.jpg\"\n",
    "    ],\n",
    "    \"putin\": [\n",
    "        \"./images/putin/putin1.jpg\"\n",
    "    ],\n",
    "    \"biden\": [\n",
    "        \"./images/biden/biden1.jpg\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "encodings = []\n",
    "names = []\n",
    "\n",
    "for person, image_paths in face_images.items():\n",
    "    for path in image_paths:\n",
    "        face = face_recognition.load_image_file(path)\n",
    "        face_bbs = face_recognition.face_locations(face)\n",
    "        if len(face_bbs) != 1:\n",
    "            print(f\"Can't analyze image at path {path} of {person}\")\n",
    "            continue\n",
    "        face_encoding = face_recognition.face_encodings(face)[0]\n",
    "        encodings.append(face_encoding)\n",
    "        names.append(person)\n",
    "\n",
    "# svm_clf = svm.LinearSVC()\n",
    "svm_clf = svm.SVC(probability=True, gamma='scale')\n",
    "svm_clf.fit(encodings, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(222, 696, 407, 510)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = face_recognition.load_image_file('./anime.jpg')\n",
    "test_face_locations = face_recognition.face_locations(test_image)\n",
    "test_face_locations\n",
    "test_face_encoding = face_recognition.face_encodings(test_image, known_face_locations=test_face_locations)\n",
    "test_face_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15269653,  0.10987215,  0.02744502,  0.01149364, -0.12665196,\n",
       "       -0.03977503,  0.04482668, -0.17497881,  0.0678868 , -0.08996343,\n",
       "        0.18113542, -0.08842672, -0.36437851, -0.07972416,  0.0031369 ,\n",
       "        0.12855591, -0.13808133, -0.15942805, -0.20948088, -0.12257048,\n",
       "        0.03341122,  0.00479252, -0.03267508, -0.03947413, -0.10862232,\n",
       "       -0.22754619, -0.05384813, -0.13053878, -0.00486688, -0.09233195,\n",
       "        0.06154098, -0.00275521, -0.20462567, -0.12419596, -0.00184974,\n",
       "       -0.00778652, -0.09120831, -0.06746903,  0.18022443, -0.02576419,\n",
       "       -0.16877732, -0.02345786,  0.03031982,  0.20937589,  0.21699429,\n",
       "        0.01485616, -0.00997108, -0.15936315,  0.10122295, -0.28451821,\n",
       "       -0.01571452,  0.14753629,  0.16091141,  0.11327104,  0.06429446,\n",
       "       -0.1358415 ,  0.05266336,  0.13518067, -0.22139801,  0.06652847,\n",
       "        0.0726627 , -0.16993606, -0.03742438, -0.04675952,  0.11315753,\n",
       "        0.07875495, -0.01854436, -0.1279642 ,  0.27543205, -0.17766093,\n",
       "       -0.14538516,  0.06727489, -0.04343649, -0.18686567, -0.32337564,\n",
       "       -0.04797696,  0.37452868,  0.17795177, -0.20764583, -0.08636312,\n",
       "       -0.11255772, -0.02844054,  0.04360618,  0.01635136, -0.06080875,\n",
       "       -0.13668099, -0.09417532,  0.01357761,  0.23730057, -0.08921993,\n",
       "       -0.02807299,  0.21355362,  0.06164951, -0.07093246,  0.05091743,\n",
       "        0.01968907, -0.10425757, -0.03058111, -0.112283  , -0.08677803,\n",
       "        0.05201192, -0.13209684,  0.0352971 ,  0.12455048, -0.15316857,\n",
       "        0.18386588, -0.02222212, -0.04379151, -0.07991353, -0.11898702,\n",
       "        0.00397348,  0.0838037 ,  0.19195668, -0.16274409,  0.24924639,\n",
       "        0.2291431 , -0.03115819,  0.10456143,  0.0139167 ,  0.03273824,\n",
       "       -0.05586712, -0.06696799, -0.22211415, -0.16730547,  0.03460846,\n",
       "        0.03031897,  0.02079945,  0.05183329])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = face_recognition.load_image_file('./images/trump/trump1.jpg')\n",
    "test_face_locations = face_recognition.face_locations(test_image)\n",
    "test_face_encoding = face_recognition.face_encodings(test_image)[0]\n",
    "test_face_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.1251586377620697,\n",
       " 0.10279892385005951,\n",
       " 0.08014370501041412,\n",
       " -0.12285250425338745,\n",
       " -0.167522594332695,\n",
       " -0.08154484629631042,\n",
       " -0.13406755030155182,\n",
       " -0.09916097670793533,\n",
       " 0.17467071115970612,\n",
       " -0.1265711784362793,\n",
       " 0.2022881805896759,\n",
       " -0.02566657029092312,\n",
       " -0.22026726603507996,\n",
       " -0.07307475060224533,\n",
       " -0.024724824354052544,\n",
       " 0.276022732257843,\n",
       " -0.18145090341567993,\n",
       " -0.1534252166748047,\n",
       " -0.019235659390687943,\n",
       " -0.06835337728261948,\n",
       " 0.0388641394674778,\n",
       " 0.02689986303448677,\n",
       " 0.015718869864940643,\n",
       " 0.11896955966949463,\n",
       " -0.1386169195175171,\n",
       " -0.36444219946861267,\n",
       " -0.0946066603064537,\n",
       " -0.11590753495693207,\n",
       " -0.17955169081687927,\n",
       " -0.024422235786914825,\n",
       " -0.059644460678100586,\n",
       " 0.02391854301095009,\n",
       " -0.22474715113639832,\n",
       " -0.07375185936689377,\n",
       " 0.08006671071052551,\n",
       " 0.1253444105386734,\n",
       " -0.0012762227561324835,\n",
       " -0.08967531472444534,\n",
       " 0.11196378618478775,\n",
       " -0.0031091044656932354,\n",
       " -0.2187918871641159,\n",
       " -0.01810726523399353,\n",
       " 0.17381441593170166,\n",
       " 0.1827765703201294,\n",
       " 0.19378970563411713,\n",
       " 0.019672608003020287,\n",
       " 0.045158687978982925,\n",
       " -0.041592035442590714,\n",
       " 0.15396469831466675,\n",
       " -0.2755257189273834,\n",
       " 0.011159158311784267,\n",
       " 0.08390963077545166,\n",
       " 0.11049928516149521,\n",
       " 0.030408218502998352,\n",
       " 0.06875497847795486,\n",
       " -0.15484844148159027,\n",
       " 0.08330164104700089,\n",
       " 0.17169323563575745,\n",
       " -0.2268952876329422,\n",
       " -0.01888200081884861,\n",
       " 0.0501033216714859,\n",
       " -0.13915790617465973,\n",
       " -0.07865022122859955,\n",
       " 0.0005927732563577592,\n",
       " 0.375878244638443,\n",
       " 0.23805288970470428,\n",
       " -0.15072087943553925,\n",
       " -0.1285034865140915,\n",
       " 0.13365016877651215,\n",
       " -0.12941403687000275,\n",
       " -0.06394254416227341,\n",
       " 0.014090178534388542,\n",
       " -0.15725618600845337,\n",
       " -0.16484396159648895,\n",
       " -0.3563081920146942,\n",
       " -0.023335859179496765,\n",
       " 0.36558476090431213,\n",
       " 0.10813068598508835,\n",
       " -0.21299493312835693,\n",
       " 0.0624789334833622,\n",
       " -0.0840119794011116,\n",
       " 0.09919943660497665,\n",
       " 0.01742297038435936,\n",
       " 0.11569869518280029,\n",
       " -0.03654037043452263,\n",
       " 0.0673573911190033,\n",
       " -0.12116142362356186,\n",
       " 0.02500082366168499,\n",
       " 0.2195691615343094,\n",
       " -0.04604307934641838,\n",
       " 0.025566022843122482,\n",
       " 0.27531129121780396,\n",
       " 0.0003654643951449543,\n",
       " 0.057928334921598434,\n",
       " 0.04330369085073471,\n",
       " 0.03525396063923836,\n",
       " -0.10564461350440979,\n",
       " -0.1107894778251648,\n",
       " -0.13513456284999847,\n",
       " -0.03288784995675087,\n",
       " 0.014590742997825146,\n",
       " -0.0008643654873594642,\n",
       " -0.021629491820931435,\n",
       " 0.08869799971580505,\n",
       " -0.24153463542461395,\n",
       " 0.01837129518389702,\n",
       " -0.07278293371200562,\n",
       " -0.04294222220778465,\n",
       " -0.09953217208385468,\n",
       " -0.013683649711310863,\n",
       " -0.038100771605968475,\n",
       " -0.06823326647281647,\n",
       " 0.08277709037065506,\n",
       " -0.21790243685245514,\n",
       " 0.18495875597000122,\n",
       " 0.1409347951412201,\n",
       " -0.04723134636878967,\n",
       " 0.18411210179328918,\n",
       " 0.053295865654945374,\n",
       " 0.023488765582442284,\n",
       " -0.05202868953347206,\n",
       " -0.11433465778827667,\n",
       " -0.10179934650659561,\n",
       " -0.05965042486786842,\n",
       " 0.10381873697042465,\n",
       " -0.051547855138778687,\n",
       " 0.1406235247850418,\n",
       " 0.01936209946870804]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_face_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.53846055e-01,  6.13332242e-02,  5.18904701e-02, -1.24199219e-01,\n",
       "       -1.16682023e-01, -1.00067154e-01, -1.05487488e-01, -6.65155649e-02,\n",
       "        1.72323138e-01, -1.38931900e-01,  1.95000827e-01, -2.45214347e-02,\n",
       "       -2.46265769e-01, -8.12552199e-02, -2.96929833e-02,  2.86456198e-01,\n",
       "       -1.71053812e-01, -1.33136079e-01, -3.54465283e-02, -4.07256484e-02,\n",
       "        8.79964381e-02,  3.25876400e-02,  5.08354604e-02,  1.03533037e-01,\n",
       "       -1.23551592e-01, -3.72065008e-01, -1.24075949e-01, -8.34776536e-02,\n",
       "       -1.07404530e-01, -2.86892280e-02, -6.64995164e-02,  1.39128836e-02,\n",
       "       -2.29611740e-01, -6.03579432e-02,  5.35075925e-02,  9.99522060e-02,\n",
       "        8.65654074e-05, -1.16375111e-01,  1.44729450e-01,  4.94986437e-02,\n",
       "       -2.42517188e-01, -4.53049727e-02,  1.28711149e-01,  1.79457396e-01,\n",
       "        1.66822866e-01,  4.22745571e-02,  2.34570932e-02, -5.58876432e-02,\n",
       "        1.27249390e-01, -2.62346476e-01, -9.75659769e-03,  1.08664043e-01,\n",
       "        1.64192896e-02,  4.55213897e-02,  6.60196245e-02, -1.05020642e-01,\n",
       "        1.38697088e-01,  1.48665458e-01, -2.61562914e-01, -6.84359446e-02,\n",
       "        5.84008023e-02, -1.22998923e-01, -7.85408616e-02, -2.46204715e-02,\n",
       "        3.40129524e-01,  1.87817737e-01, -1.40786424e-01, -1.34534106e-01,\n",
       "        1.76187173e-01, -1.27364993e-01, -4.50569429e-02,  1.60727948e-02,\n",
       "       -1.50385126e-01, -1.63846314e-01, -3.37438315e-01, -5.46752848e-03,\n",
       "        3.64943594e-01,  1.35758728e-01, -1.38977855e-01,  1.24585398e-01,\n",
       "       -6.49083704e-02,  4.58826013e-02,  6.51599318e-02,  1.23794496e-01,\n",
       "       -2.13619284e-02,  7.94594660e-02, -9.13027152e-02,  2.58933175e-02,\n",
       "        1.80924818e-01, -2.47490164e-02,  2.84806341e-02,  2.57637590e-01,\n",
       "        9.77712777e-03,  7.56968856e-02,  6.69860318e-02,  4.39015701e-02,\n",
       "       -1.02937646e-01, -8.79815072e-02, -1.30717888e-01,  5.05617214e-03,\n",
       "        3.34874690e-02, -1.32932356e-02, -1.34143997e-02,  8.33467171e-02,\n",
       "       -1.71651468e-01,  4.25577164e-02, -4.19344828e-02, -5.65714538e-02,\n",
       "       -8.48146155e-02, -2.85350555e-03, -7.19453916e-02, -7.13668466e-02,\n",
       "        1.54125635e-02, -2.20401332e-01,  2.04973042e-01,  1.51466087e-01,\n",
       "       -1.90525036e-02,  1.49220988e-01,  7.73977339e-02,  5.69722094e-02,\n",
       "       -5.85370027e-02, -8.08175802e-02, -1.44082919e-01, -7.76207596e-02,\n",
       "        1.29951209e-01, -4.55829501e-02,  1.39252737e-01,  6.62557222e-03])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparing_face = face_recognition.load_image_file('./images/huou/huou.jpg')\n",
    "comparing_face_locations = face_recognition.face_locations(comparing_face)\n",
    "comparing_face_encoding = face_recognition.face_encodings(comparing_face)[0]\n",
    "comparing_face_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "face_recognition.compare_faces([test_face_encoding], np.array([-1.53846055e-01,  6.13332242e-02,  5.18904701e-02, -1.24199219e-01,\n",
    "       -1.16682023e-01, -1.00067154e-01, -1.05487488e-01, -6.65155649e-02,\n",
    "        1.72323138e-01, -1.38931900e-01,  1.95000827e-01, -2.45214347e-02,\n",
    "       -2.46265769e-01, -8.12552199e-02, -2.96929833e-02,  2.86456198e-01,\n",
    "       -1.71053812e-01, -1.33136079e-01, -3.54465283e-02, -4.07256484e-02,\n",
    "        8.79964381e-02,  3.25876400e-02,  5.08354604e-02,  1.03533037e-01,\n",
    "       -1.23551592e-01, -3.72065008e-01, -1.24075949e-01, -8.34776536e-02,\n",
    "       -1.07404530e-01, -2.86892280e-02, -6.64995164e-02,  1.39128836e-02,\n",
    "       -2.29611740e-01, -6.03579432e-02,  5.35075925e-02,  9.99522060e-02,\n",
    "        8.65654074e-05, -1.16375111e-01,  1.44729450e-01,  4.94986437e-02,\n",
    "       -2.42517188e-01, -4.53049727e-02,  1.28711149e-01,  1.79457396e-01,\n",
    "        1.66822866e-01,  4.22745571e-02,  2.34570932e-02, -5.58876432e-02,\n",
    "        1.27249390e-01, -2.62346476e-01, -9.75659769e-03,  1.08664043e-01,\n",
    "        1.64192896e-02,  4.55213897e-02,  6.60196245e-02, -1.05020642e-01,\n",
    "        1.38697088e-01,  1.48665458e-01, -2.61562914e-01, -6.84359446e-02,\n",
    "        5.84008023e-02, -1.22998923e-01, -7.85408616e-02, -2.46204715e-02,\n",
    "        3.40129524e-01,  1.87817737e-01, -1.40786424e-01, -1.34534106e-01,\n",
    "        1.76187173e-01, -1.27364993e-01, -4.50569429e-02,  1.60727948e-02,\n",
    "       -1.50385126e-01, -1.63846314e-01, -3.37438315e-01, -5.46752848e-03,\n",
    "        3.64943594e-01,  1.35758728e-01, -1.38977855e-01,  1.24585398e-01,\n",
    "       -6.49083704e-02,  4.58826013e-02,  6.51599318e-02,  1.23794496e-01,\n",
    "       -2.13619284e-02,  7.94594660e-02, -9.13027152e-02,  2.58933175e-02,\n",
    "        1.80924818e-01, -2.47490164e-02,  2.84806341e-02,  2.57637590e-01,\n",
    "        9.77712777e-03,  7.56968856e-02,  6.69860318e-02,  4.39015701e-02,\n",
    "       -1.02937646e-01, -8.79815072e-02, -1.30717888e-01,  5.05617214e-03,\n",
    "        3.34874690e-02, -1.32932356e-02, -1.34143997e-02,  8.33467171e-02,\n",
    "       -1.71651468e-01,  4.25577164e-02, -4.19344828e-02, -5.65714538e-02,\n",
    "       -8.48146155e-02, -2.85350555e-03, -7.19453916e-02, -7.13668466e-02,\n",
    "        1.54125635e-02, -2.20401332e-01,  2.04973042e-01,  1.51466087e-01,\n",
    "       -1.90525036e-02,  1.49220988e-01,  7.73977339e-02,  5.69722094e-02,\n",
    "       -5.85370027e-02, -8.08175802e-02, -1.44082919e-01, -7.76207596e-02,\n",
    "        1.29951209e-01, -4.55829501e-02,  1.39252737e-01,  6.62557222e-03]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20796845, 0.19955125, 0.19751443, 0.19787987, 0.19708601],\n",
       "       [0.20837915, 0.20013739, 0.20409076, 0.19272866, 0.19466404],\n",
       "       [0.20770625, 0.20040839, 0.19260172, 0.20210602, 0.19717762]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_people_image = face_recognition.load_image_file('./multiple-people.jpg')\n",
    "multiple_people_face_locations = face_recognition.face_locations(multiple_people_image)\n",
    "face_features = face_recognition.face_encodings(multiple_people_image, known_face_locations=multiple_people_face_locations)\n",
    "# for face_bb in multiple_people_face_locations:\n",
    "#     top, right, bottom, left = face_bb\n",
    "#     print(face_features)\n",
    "face_features\n",
    "svm_clf.predict(face_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/svm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/svm_model.pkl', 'rb') as f:\n",
    "    saved_svm_clf = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biden']\n"
     ]
    }
   ],
   "source": [
    "print(saved_svm_clf.predict([test_face_encoding]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
